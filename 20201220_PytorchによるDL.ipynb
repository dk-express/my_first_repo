{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorとArrayの型を比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num1 : (3, 4) / float64\n",
      "ten1 : torch.Size([2, 3]) / torch.float32\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "num1 = np.zeros((3, 4))\n",
    "ten1 =torch.zeros((2, 3))\n",
    "print(f\"num1 : {num1.shape} / {num1.dtype}\") #arrayは実数はfloat64。整数がint32\n",
    "print(f\"ten1 : {ten1.shape} / {ten1.dtype}\") #tensorは実数はfloat32。整数はint64\n",
    "print(type(ten1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Array とTensorの変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "変換前：tensor([0., 0., 0.])\n",
      "変換後：[0. 0. 0.]\n",
      "再変換後：tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "before = torch.zeros(3)\n",
    "print(f\"変換前：{before}\")\n",
    "\n",
    "after = before.numpy() #TensorからArrayへ変換 → .numpy メソッド！！\n",
    "print(f\"変換後：{after}\")\n",
    "\n",
    "data = torch.from_numpy(after) #ArrayからTensorへ変換 → torch.from_numpy メソッド！！\n",
    "print(f\"再変換後：{data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "変換前：tensor([1., 1., 1.])\n",
      "変換後：[1. 1. 1.]\n",
      "再変換後：tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#この時、メモリが共有されていることに注意する！！\n",
    "before.add_(1)\n",
    "print(f\"変換前：{before}\")\n",
    "print(f\"変換後：{after}\")\n",
    "print(f\"再変換後：{data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自動微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    " x = torch.tensor(1.0, requires_grad = True) # 1.0はx = 1の時の出力を表す。requires_gradは自動微分機能に関する記述。\n",
    "a, b = 3, 5\n",
    "y = a*x + b  # y = 3x + 5  これ、順伝播。\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()  #計算後のTensor名.backward() で微分が行われる。これ、逆伝播。\n",
    "# Tensorで宣言されていない変数は微分できないので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)  #x.gradはy = 3x + 5 をxで微分した値＝３。勾配の計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "y = a*x + b\n",
    "y.backward()\n",
    "print(x.grad) #backward()が実行される度に、Tensor名.gradは加算を繰り返してしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# 例題演習\n",
    "v = torch.tensor(1.0, requires_grad = True)\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "out = 4*v + 6 *w + 1\n",
    "\n",
    "out.backward()\n",
    "print(v.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transposeとreshape : 次元の調整に使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5239, 0.4946, 0.4096],\n",
      "        [0.2965, 0.0912, 0.7677],\n",
      "        [0.9280, 0.3570, 0.7567],\n",
      "        [0.5698, 0.2118, 0.7391]])\n",
      "tensor([[0.5239, 0.2965, 0.9280, 0.5698],\n",
      "        [0.4946, 0.0912, 0.3570, 0.2118],\n",
      "        [0.4096, 0.7677, 0.7567, 0.7391]])\n",
      "tensor([[0.5239, 0.2965, 0.9280, 0.5698],\n",
      "        [0.4946, 0.0912, 0.3570, 0.2118],\n",
      "        [0.4096, 0.7677, 0.7567, 0.7391]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((4, 3))\n",
    "print(x)\n",
    "print(x.transpose(0, 1)) #どっちでも転置\n",
    "print(torch.t(x))              #どっちでも転置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(16)\n",
    "print(x)\n",
    "print(x.reshape(2, 8))\n",
    "print(x.reshape(4, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero_()関数 ：行列を全てゼロに書き換える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a)\n",
    "a.zero_()  #\"_\"はインプレース（破壊的メソッド）を表す。変数の値を変更してしまうので注意が必要。\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全結合層と活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全結合層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn #全結合層のため\n",
    "from torch.nn import functional as F #活性化関数のため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4044,  0.2114], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 全結合層 fc を定義して、Tensor[1, 2, 3, 4] を伝播させる。\n",
    "# 4次元ベクトルを2次元に変換する全結合層をクラスとして定義\n",
    "fc = nn.Linear(4, 2)      # 入力が4, 出力が2の全結合層。４つの数値から２つの数値を計算してくれる「いい感じの何か」.\n",
    "x = torch.Tensor([1, 2, 3, 4])\n",
    "\n",
    "# fc の__call__()メソッドを呼ぶことでx を伝播できる。\n",
    "x = fc(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[説明]\n",
    "・tensor([x, y]) のx, yは[1,2, 3, 4] をfc_layerの初期パラメータで変換した値。\n",
    " 4次元tensorが２次元に変換されていることがわかる。\n",
    "・fc をインスタンスとして宣言し、__call__() で伝播させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 活性化関数：relu関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.5000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# 次に、活性化関数を実装する。その前に、relu() 関数を理解。\n",
    "x = torch.Tensor([-1.0, -0.5, 0.5, 1.0])\n",
    "x = F. relu(x)\n",
    "print(x)  #０以下の値が「0.0000」となっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0744, -0.3606], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "### これで順伝播は終わり！\n",
    "# x とfc の用意。\n",
    "x = torch.Tensor([-2, -1, 1, 2])\n",
    "fc = nn.Linear(4, 2)\n",
    "\n",
    "# 全結合層を通じたx を表示。\n",
    "print(fc(x))\n",
    "\n",
    "# さらに、relu 関数を適用したx を表示。\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数と最適化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torchvision.models import vgg11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まずは、損失関数をクラスとして用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 下記２つのTensorのMSE（平均二乗誤差）を求める。\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "y = torch.Tensor([1, -1, 0])\n",
    "\n",
    "loss = criterion(x, y)\n",
    "\n",
    "print(loss)  # たったこれだけで　MSEが計算できてしまう！すごい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 次に、最適化関数を実装する →今回はAdamを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルを用意する\n",
    "model = vgg11()\n",
    "\n",
    "# 用意したモデルのパラメーターを与えて、クラスを定義する。\n",
    "optimizer = optim.Adam(model.parameters())  #Adamには、パラメーターが必要！\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数MSEを宣言して、損失を計算してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "x = torch.Tensor([1, 1, 1, 1])\n",
    "y = torch.Tensor([0, 2, 4, 6])\n",
    "\n",
    "loss = criterion(x, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初期化関数\"__init__()\"で、モデルに使うレイヤーや関数を準備し、forward()関数でモデルを組み立てる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):      #nn.Moduleが基底クラス、mlp_netが派生クラス。\n",
    "    def __init__(self):\n",
    "        super(). __init__ ()         # 基底クラスのデータ属性を初期化。下記のself.fc1, self.fc2.....たちを外から代入するデータ属性として設定。\n",
    "        \n",
    "        #全結合層を２つ\n",
    "        self.fc1 = nn.Linear(3, 5) #入力層が3次元、中間層が５次元、出力層が２次元\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "        \n",
    "        #損失関数と最適化関数\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())  #Adamのパラメーター設定は、nn.Moduleが勝手にやってくれるので、心配なし！\n",
    "    \n",
    "    def forward(self, x) :             # forward()メソッドの定義。外部引数はx. これが順伝播。\n",
    "        x = self.fc1(x)\n",
    "        print('[fc1を通過]\\n', x)    #中間層の出力を表示\n",
    "        \n",
    "        #  活性化関数\n",
    "        x = F.relu(x)\n",
    "        print('[relu()を通過]\\n', x)  # relu()の適用結果を表示\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 作成したモデルに、tensor[0, 1, 2]を伝播させてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fc1を通過]\n",
      " tensor([ 0.0548,  1.4792, -1.5062, -0.5882, -1.1932], grad_fn=<AddBackward0>)\n",
      "[relu()を通過]\n",
      " tensor([0.0548, 1.4792, 0.0000, 0.0000, 0.0000], grad_fn=<ReluBackward0>)\n",
      "[モデルの出力]\n",
      " tensor([0.1737, 0.6141], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# モデルを宣言\n",
    "model = mlp_net()\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "\n",
    "# xを伝播させる\n",
    "output = model(x)\n",
    "\n",
    "print('[モデルの出力]\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[例題]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):      #nn.Moduleが基底クラス（使用必須）、mlp_netが派生クラス。\n",
    "    def __init__(self):\n",
    "        super(). __init__ ()         # 基底クラスのデータ属性を初期化。下記のself.fc1, self.fc2.....たちを外から代入するデータ属性として設定。\n",
    "        \n",
    "        #全結合層を２つ\n",
    "        self.fc1 = nn.Linear(4, 3) #入力層が3次元、中間層が５次元、出力層が２次元\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "        \n",
    "        #損失関数と最適化関数\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())  #Adamのパラメーター設定は、nn.Moduleが勝手にやってくれるので、心配なし！\n",
    "    \n",
    "    def forward(self, x) :             # forward()メソッドの定義。外部引数はx. これが順伝播。\n",
    "        x = self.fc1(x)\n",
    "        print('[fc1を通過]\\n', x)    #中間層の出力を表示\n",
    "        \n",
    "        #  活性化関数\n",
    "        x = F.relu(x)\n",
    "        print('[relu()を通過]\\n', x)  # relu()の適用結果を表示\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_net(\n",
       "  (fc1): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainset = datasets.MNIST(root=\\'./data\\', train=True, download=True, transform=transforms.ToTensor()) \\n #学習用データセット\\ntrain_loader = utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)  \\n # ミニバッチごとにデータを纏める(学習時にはshuffle=True)\\n\",\\n\\ntestset = datasets.MNIST(root=\\'./data\\', train=False, download=True, transform=transforms.ToTensor()) \\n # 検証用データセット\\ntest_loader = utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2) \\n # ミニバッチごとにデータを纏める(学習時にはshuffle=False)\\n '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) \n",
    " #学習用データセット\n",
    "train_loader = utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)  \n",
    " # ミニバッチごとにデータを纏める(学習時にはshuffle=True)\\n\",\n",
    " # DataLoder : ミニバッチを指定したら、勝手にデータを分けてくれる便利なもの。\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) \n",
    " # 検証用データセット\n",
    "test_loader = utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2) \n",
    " # ミニバッチごとにデータを纏める(学習時にはshuffle=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):      #nn.Moduleが基底クラス、mlp_netが派生クラス。\n",
    "    def __init__(self):\n",
    "        super(). __init__ ()         # 基底クラスのデータ属性を初期化。下記のself.fc1, self.fc2.....たちを外から代入するデータ属性として設定。\n",
    "        \n",
    "        #全結合層を２つ\n",
    "        self.fc1 = nn.Linear(784, 512) #入力層が784次元、中間層が５12次元、出力層が10次元\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "        #損失関数と最適化関数\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())  #Adamのパラメーター設定は、nn.Moduleが勝手にやってくれるので、心配なし！\n",
    "    \n",
    "    def forward(self, x) :             # forward()メソッドの定義。外部引数はx. これが順伝播。\n",
    "        x = self.fc1(x)\n",
    "        #  活性化関数\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意点\n",
    "①　２次元の画像であるテンソルを、１次元のテンソルに変換する\\\n",
    "→ reshape()を使う！！\\\n",
    "②　損失を求めるために正解データをone-hotベクトル化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loder):\n",
    "    \n",
    "    model.train() # 今は学習中であることを明治するコード！！\n",
    "    \n",
    "    # ミニバッチごとにループさせる,train_loaderの中身を出し切ったら1エポックとなる\n",
    "    for batch_imgs, batch_labels in train_loader:\n",
    "        batch_imgs = batch_imgs.reshape(-1, 28*28*1)  # 画像データを1次元に変換\n",
    "        labels = torch.eye(10)[batch_labels]  # 正解ラベルをone-hotベクトルへ変換\n",
    "\n",
    "        outputs = model(batch_imgs)  # 順伝播\n",
    "        model.optimizer.zero_grad()  # 勾配を初期化！！（前回のループ時の勾配を削除。backwardよりも前で宣言する必要あり）\n",
    "        loss = model.criterion(outputs, labels)  # outputs と labelsの間の損失を計算\n",
    "        loss.backward()  # 逆伝播で勾配を計算\n",
    "        model.optimizer.step()  # 最適化（.step とすると最適化される）\n",
    "    return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 色々な活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-26-4545548c67fc>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-4545548c67fc>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    \"\"\"softmax関数 : 入力ベクトルの要素が０以上で、和が１になるようにスケール処理。：つまり、確率値に直すということ。\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.tensor([-2.0, 1.0, 4.0, 0.0])\n",
    "\n",
    "print('relu :', F.relu(x))   # relu関数\n",
    "\n",
    "print('softmax:', F.softmax(x, dim = 0)) \n",
    "　\"\"\"softmax関数 : 入力ベクトルの要素が０以上で、和が１になるようにスケール処理。：つまり、確率値に直すということ。\n",
    "      １次元のベクトルのsoftmax関数を適用する場合は、引数dimに0を指定する。\"\"\"\n",
    "    \n",
    "print('sigmoid:', torch.sigmoid(x))  #sigmoid関数 : 入力値を0から1に収まるように処理する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 色々な損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ①MSELoss関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・２つのベクトルの要素ごとの差の２上から誤差を算出。\\\n",
    "・インスタンス宣言時に、引数\"reduction = mean\"で要素ごとの平均、\"reduction = sum\"で合計を損失値として出力。\\\n",
    "・\"reduction = none\"を指定すると、平均や合計処理をする前の、要素ごとの誤差をテンソルとして出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ②CrossEntropyLoss関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・MSが「全要素の誤差を求めていたのに対し、「正解ラベルと対応する出力との差のみ」を計算。\\\n",
    "・しかし、正しくこれを用いるには、モデルの出力をsoftmax関数などで確率にする必要がある。\\\n",
    "・なぜならば、正解ラベルに対するモデルの出力が「１」の時に損失が０となってしまうからだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(0.1267)\n",
      "CrossEntropy: tensor(0.9398)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# バッチサイズ=1 と見立てたデータを用意\n",
    "x = torch.tensor([[0.2, 0.5, 0.3]])  #MSELossの場合は、入力の合計値が1になるように準備する必要がある。\n",
    "mse_label = torch.tensor([[0, 1, 0]])  #crossentropyの場合は、上記のような制約はない。\n",
    "cel_label = torch .tensor([1])\n",
    "\n",
    "print('MSE:', nn.MSELoss(reduction = 'mean')(x, mse_label))\n",
    "print('CrossEntropy:', nn.CrossEntropyLoss()(x, cel_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/24 Pytorchチュートリアル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# long型（int型）の数値0で初期化された行列を生成\n",
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接、数値を指定して行列を生成。（pythonのリスト型による作成）\n",
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.5100, -0.6161,  0.8510],\n",
      "        [-0.0251, -0.2915, -0.1122],\n",
      "        [-0.5384, -0.0136,  0.2812],\n",
      "        [-0.2740, -1.0454, -0.8488],\n",
      "        [ 0.9613, -0.9050, -0.4126]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# すでにあるtensorを元に、新しくtensorを生成\n",
    "x = x.new_ones(5, 3, dtype = torch.double)  # new_ * methods\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)  #_like で形状（５行３列）を継承できる。\n",
    "    # randn : 平均が０、分散が１の乱数。\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🌟 torch.doubleはtorch.float64と同じ意味で、64bitの浮動小数点型を表す\n",
    "尚、torch.half = torch.float16, torch.float = torch.float32\n",
    "     torch.short = torch.int16, torch.int = torch.int32, torch.long = torch.int64 を表す。\\\n",
    "     そして、基本的に32bitのtorch.float か torch.int しか使わない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （注意）Numpyの浮動小数点の数値が、デフォルトで64bitになっている点に注意。\\\n",
    "したがって、Numpyから変換したtensorは、\n",
    "#### float() メソッド\n",
    "を呼び出してデータ型の変換を行う必要がある！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorの演算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5100, -0.6161,  0.8510],\n",
       "        [-0.0251, -0.2915, -0.1122],\n",
       "        [-0.5384, -0.0136,  0.2812],\n",
       "        [-0.2740, -1.0454, -0.8488],\n",
       "        [ 0.9613, -0.9050, -0.4126]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1686, 0.2283, 0.9823],\n",
       "        [0.5158, 0.4037, 0.5889],\n",
       "        [0.7457, 0.0056, 0.5473],\n",
       "        [0.2205, 0.7648, 0.8726],\n",
       "        [0.4483, 0.9911, 0.1036]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6785, -0.3879,  1.8333],\n",
      "        [ 0.4907,  0.1122,  0.4767],\n",
      "        [ 0.2073, -0.0079,  0.8284],\n",
      "        [-0.0534, -0.2806,  0.0238],\n",
      "        [ 1.4096,  0.0861, -0.3091]])\n"
     ]
    }
   ],
   "source": [
    "# 足し算①\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6785, -0.3879,  1.8333],\n",
      "        [ 0.4907,  0.1122,  0.4767],\n",
      "        [ 0.2073, -0.0079,  0.8284],\n",
      "        [-0.0534, -0.2806,  0.0238],\n",
      "        [ 1.4096,  0.0861, -0.3091]])\n"
     ]
    }
   ],
   "source": [
    "# 足し算②\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2084, -2.2363,  4.3863],\n",
       "        [ 0.4155, -0.7624,  0.1400],\n",
       "        [-1.4080, -0.0487,  1.6719],\n",
       "        [-0.8753, -3.4168, -2.5225],\n",
       "        [ 4.2934, -2.6289, -1.5469]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x) #破壊的メソッド\"_\" により、x + y が y に代入された。←←これ、実行のたびに足し続けちゃうんだけど・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スライシング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1167, -2.0825, -0.0997,  0.7579],\n",
       "        [ 0.1968,  0.9784,  0.0714,  0.4933],\n",
       "        [-1.5637, -0.4070, -0.3432, -2.0151],\n",
       "        [-1.3312, -0.2478,  0.6550, -1.4580]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1167, -2.0825, -0.0997,  0.7579,  0.1968,  0.9784,  0.0714,  0.4933,\n",
       "        -1.5637, -0.4070, -0.3432, -2.0151, -1.3312, -0.2478,  0.6550, -1.4580])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(16) #１６個のベクトル？    .viewメソッドで、tensorの形を変えられる。\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1167, -2.0825, -0.0997,  0.7579,  0.1968,  0.9784,  0.0714,  0.4933],\n",
       "        [-1.5637, -0.4070, -0.3432, -2.0151, -1.3312, -0.2478,  0.6550, -1.4580]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(-1, 8) #２行８列\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Numpyとの接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
